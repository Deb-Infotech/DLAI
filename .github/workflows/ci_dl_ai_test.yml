# This workflow will install Python dependencies, run tests and lint with a single version of Python
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python

name: DL AI CI Test Execution

on:
  workflow_dispatch: # Manual Execution From GitHub Actions
  # Enable below push if automated execution required
  push:
    #    branches: [ "master" ] " Applicable on all branches
    branches-ignore: [ "temp*" ]

  pull_request:
    branches: [ "master", "develop" ]

permissions:
  contents: write
  pages: write
  checks: write
  id-token: write

jobs:
  build:

    runs-on: ubuntu-latest

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python 3.10
        uses: actions/setup-python@v3
        with:
          python-version: "3.10"

      - name: Cache Python dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Create Virtual Environment
        run: |
          echo "Creating virtual environment..."
          # pip install virtualenv # Not required as virtualenv is part of standard library post python 3.3
          # echo "Ran: pip install virtualenv"
          python --version
          python -m venv venv
          echo "Ran: python -m venv venv"
          source venv/bin/activate
          echo "Virtual environment created..."

      - name: Install dependencies In Virtual Environment
        run: |
          source venv/bin/activate
          echo "Ran: source venv/bin/activate"
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install flake8

      - name: Linting Coding Best Practices With flake8
        run: |
          source venv/bin/activate
          # stop the build if there are Python syntax errors or undefined names
          flake8 . --exclude=venv --count --select=E9,F63,F7,F82 --show-source --statistics
          # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
          flake8 . --exclude=venv --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      - name: Execute Tests with pytest
        run: |
          source venv/bin/activate
          pytest

      - name: Archive JUnit And PyTest HTML Test Result
        uses: actions/upload-artifact@v4
        with:
          name: JUnit And PyTest HTML Test Result
          path: |
            junit/
            reports/

      - name: Publish PyTest Junit Style Report
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: PyTest Test Result (JUnit Style)
          path: junit/report.xml
          reporter: java-junit
          # junit plugin will represent test results from junit/report.xml
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Deploy reports folder to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        # this will dump contents of ./reports folder to branch - gh-pages
        # GitHub Pages under repository is configured to have a live site build on content in branch - gh-pages
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./reports
          commit_message: "Deploy pytest html reports to GitHub Pages"

      - name: Add html report link to summary
        uses: actions/github-script@v6
        with:
          script: |
            const owner = process.env.GITHUB_REPOSITORY_OWNER;
            const repo = process.env.GITHUB_REPOSITORY.split('/')[1];
            const html_reportUrl = `https://${owner}.github.io/${repo}/report.html`;
            const dashboard_reportUrl = `https://${owner}.github.io/${repo}/report_dashboard.html`;
            core.summary
              .addHeading('PyTest HTML Report')
              .addLink('View PyTest HTML Report', html_reportUrl)
              .addRaw('\n')
              .addLink('View PyTest HTML Dashboard Report', dashboard_reportUrl)
              .write();
      

         
        





